---
title: "Homework 3"
author: "Jenesis Merriman"
date: "October 15, 2022"
output: github_document
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(collapse = TRUE, message = FALSE)

library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis")

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r load_libraries}
library(tidyverse)
library(readr)
```

### Problem 1

```{r}
data("instacart")
```

### Problem 2

This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). 

First I will load the data while cleaning names:

```{r load}
accel_data = 
  read_csv("data/accel_data.csv") %>%
  janitor::clean_names() %>%
  view()
```

Next, I will tidy the data, using `pivot_longer` to collapse all 1440 activity\* variables into two new variables 'minute' and 'activity_count'. I will also use `mutate` to create a new 'day_type' factor variable indicating the type of day (weekday vs. weekend) fo each observation, make 'minute' a double variable, and sort 'day' (days of the week) chronologically.

```{r tidy}
accel_tidy_data =
  pivot_longer(
    accel_data, 
    activity_1:activity_1440,
    names_to = "minute", 
    names_prefix = "activity_",
    values_to = "activity_count") %>%
  mutate(
    day_type = as.factor(ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday")),
    minute = as.double(minute),
    day = as.factor(ordered(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))))

view(accel_tidy_data)
```

*The resulting dataset contains `r nrow(accel_tidy_data)` rows and `r ncol(accel_tidy_data)` columns, with each row representing a single minute of accelerometer data for a 63 year-old male over the course of five weeks. Variables include week number (week), day ID (day_id), day of the week (day), minute of the day starting at midnight (minute), activity count per minute (activity_count), and whether or not the observation falls on a weekday or weekend (day_type).*

Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?***

The following code returns a table showing the total activity counts for each day by week, with days sorted chronologically from Monday-Sunday:

```{r}
accel_tidy_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) %>%
  pivot_wider(
    names_from = "day", 
    values_from = "total_activity") %>%
  knitr::kable(digits = 1)
```

Accelerometer data allows the inspection of activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.***

```{r}
accel_tidy_data %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_line() + 
    labs(
    title = "Activity time courses by day",
    x = "Minute",
    y = "Activity Count")
```

### Problem 3

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.

```{r}
data("ny_noaa")

ny_noaa = 
  ny_noaa %>% 
  as_tibble(ny_noaa)

summary(ny_noaa)
head(ny_noaa)
```

*This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns, with each row representing a single observation from a New York state weather station between January 1, 1981 and December 31, 2010. Variables include weather station ID (id), date of observation (date), precipitation in tenths of mm (prcp), snowfall in mm (snow), snow depth in mm (snwd), maximum temperature in tenths of degrees C (tmax), and minimum temperature in tenths of degrees C (tmin). Because each weather station may collect only a subset of these variables, this dataset contains substantial missing data. There are `r sum(is.na(ny_noaa))` values missing in this dataset.*

Then, do or answer the following (commenting on the results of each):

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

*To clean the data, the following code uses `separate` to create separate variables for year, month, and day and `mutate` to convert variables to reasonable types and units. Specifically, 'tmax' and 'tmin' were converted from tenths of degrees celsius to degrees celsius by transforming the variable type from character to numeric and dividing each value by 10. Similarly, 'prcp' was converted from tenths of a millimeter to millimeters by dividing each value by 10.*

```{r}
ny_noaa_tidy =
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    day = as.integer(day),
    year = as.factor(year),
    month = month.name[as.numeric(month)],
    month = factor(month, levels = month.name), 
    prcp = as.double(prcp / 10),
    tmax = as.integer(tmax),
    tmax = as.double(tmax / 10),
    tmin = as.integer(tmin),
    tmin = as.double(tmin / 10))
```

For snowfall, what are the most commonly observed values? Why?
```{r}
ny_noaa_tidy %>% 
  count(snow) %>% 
  arrange(desc(n))
```

*The most commonly observed values for snowfall are 0 mm by far, followed by NA. This is because there are significantly more days in the year with no snow (snow = 0) than days with snow. This dataset also contains extensive missing data, explaining why NA is the second most commonly observed value for snow.*

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interprettable structure? Any outliers?***

```{r}
ny_noaa_tidy %>%
  filter(
    month %in% c("January","July")) %>% 
  group_by(id, month, year) %>% 
  summarize(
    tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = tmax_mean, group = id, color = id)) +
  geom_line(show.legend = FALSE) +
  facet_grid(.~month) +
  theme(axis.text.x = element_blank(), legend.position = "none") +
  labs(
    title = "Mean max temperature over time ",
    x = "Year (1981-2010)",
    y = "Mean Max Temperature (C)")
```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
tmax_tmin_p =
  ny_noaa_tidy %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  theme(legend.position = "right")

snowfall_p =
  ny_noaa_tidy %>%
  filter(
    snow > 0, snow < 100) %>% 
  group_by(id, month, year) %>% 
  ggplot(aes(x = snow, color = year)) +
  geom_density() +
  theme(legend.position = "none") 

tmax_tmin_p / snowfall_p
```

```{r i work}
# Rectangular binning
ny_noaa_tidy %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_bin2d() +
  theme(legend.position = "right")

# Hexagonal binning - WINNER
ny_noaa_tidy %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  theme(legend.position = "right")
```

```{r ii work}
ny_noaa_tidy %>%
  filter(
    snow > 0, snow < 100) %>% 
  group_by(id, month, year) %>% 
  ggplot(aes(x = snow, fill = year)) +
  geom_density(alpha = 0.4, adjust = .5) +
  theme(legend.position = "none") 

ny_noaa_tidy %>% #WINNER
  filter(
    snow > 0, snow < 100) %>% 
  group_by(id, month, year) %>% 
  ggplot(aes(x = snow, color = year)) +
  geom_density(adjust = .5) +
  theme(legend.position = "none") 
```

