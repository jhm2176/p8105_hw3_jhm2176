---
title: "Homework 3"
author: "Jenesis Merriman"
date: "October 16, 2022"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)
library(readr)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis")

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
This problem uses the Instacart data. First, I will load the data and do some exploration.

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

**Description**:

### Part One

The following code tells us how many aisles there are, and which aisles are the most items ordered from.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
There are 134 aisles in this dataset. The most items are ordered from the fresh vegetable (n = 150609) and fresh fruit aisle (n = 150473).

### Part Two

The following code makes a plot that shows the number of items ordered in each aisle for aisles with more than 10000 items ordered. Aisles are arranged by ascending number of items ordered.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 1000) %>%
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

### Part Three
The following code returns a table showing the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

### Part Four
Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

## Problem 2

This problem uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF).

### Part One
First I will load the data while cleaning names.

```{r load}
accel_data = 
  read_csv("data/accel_data.csv") %>%
  janitor::clean_names()
```

Next, I will tidy the data, using `pivot_longer` to collapse all 1440 activity\* variables into two new variables 'minute' and 'activity_count'. I will also use `mutate` to create a new 'day_type' factor variable indicating the type of day for each observation, convert 'minute' into a double variable, and sort 'day' chronologically.

```{r tidy}
accel_tidy_data =
  pivot_longer(
    accel_data, 
    activity_1:activity_1440,
    names_to = "minute", 
    names_prefix = "activity_",
    values_to = "activity_count") %>%
  mutate(
    day_type = as.factor(ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday")),
    minute = as.double(minute),
    day = as.factor(ordered(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))))
```

**Description**: The resulting dataset contains `r nrow(accel_tidy_data)` rows and `r ncol(accel_tidy_data)` columns, with each row representing a single minute of accelerometer data for a 63 year-old male over the course of five weeks. Variables include week number (week), day ID (day_id), day of the week (day), minute of the day starting at midnight (minute), activity count per minute (activity_count), and whether or not the observation falls on a weekday or weekend (day_type).

### Part Two
Traditional analyses of accelerometer data focus on the total activity over the day. The following code returns a table showing the total activity counts (total_activity) for each day by week, with days sorted chronologically from Monday-Sunday.

```{r}
accel_tidy_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) %>%
  pivot_wider(
    names_from = "day", 
    values_from = "total_activity") %>%
  knitr::kable(digits = 1)
```
To help me better visualize trends in this data, the following code creates a plot displaying total_activity over time by day of the week.

```{r}
accel_tidy_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) %>%
  ggplot(aes(x = week, y = total_activity, color = day)) +
  geom_point() + 
  geom_line() +
   labs(
    title = "Total activity over time")
```

**Trends**: Total activity varies by day and by week. For example, total activity for Monday increases significantly from week 1 to week 3, then decreases from week 3 to week 5, while total activity for Sunday decreases steadily over time, except for a slight increase between week 2 and week 3. At the start of data collection, activity was higher at the end of the week (Friday-Sunday) than the beginning of it (Monday-Wednesday). At the end of data collection, activity counts were lowest on the weekend (Saturday-Sunday). Notably, total activity for Saturday in weeks 4 and 5 is stagnant at 1440. Since our activity variables represent activity counts for each minute of a 24-hour day and there are 1440 minutes in a day, a total_activity count of 1440 is unusual, indicating 1 activity count per minute. This may indicate that no activity was recorded on these two Saturdays, or something else, but it is impossible to know for sure without more information.

### Part Three
Accelerometer data also allows the inspection of activity over the course of the day. The following code creates a single-panel plot that shows the 24-hour activity time courses for each day, using color to indicate the day of the week.

```{r}
accel_tidy_data %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_line(alpha = 0.5) + 
    labs(
    title = "Activity time courses by day",
    x = "Minute",
    y = "Activity Count")
```

**Interpretation**: Based on this graph, most activity occurs between 7am and 10 pm. This makes sense considering most adults sleep somewhere between 10 pm and 7 am. Activity count peaks around 9 pm (minutes ≈ 1250) on weekdays and late morning (minutes ≈ 500-750) on weekends. This pattern is not unusual, considering most adults work during weekdays and may be more active after work in the evenings or during the day on weekends. 

## Problem 3

This problem uses the NY NOAA data. 

First, I will load the data and describe it.

```{r}
data("ny_noaa")

ny_noaa = 
  ny_noaa %>% 
  as_tibble(ny_noaa)
```

**Description**: This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns, with each row representing a single observation from a New York state weather station between January 1, 1981 and December 31, 2010. Variables include weather station ID (id), date of observation (date), precipitation in tenths of mm (prcp), snowfall in mm (snow), snow depth in mm (snwd), maximum temperature in tenths of degrees C (tmax), and minimum temperature in tenths of degrees C (tmin). Because each weather station may collect only a subset of these variables, this dataset contains substantial missing data. There are `r sum(is.na(ny_noaa))` missing values in this dataset.

### Part One
Next, I will clean the data. The following code uses `separate` to create separate variables for year, month, and day and `mutate` to convert variables to reasonable types and units. Specifically, 'tmax' and 'tmin' were converted from tenths of degrees celsius to degrees celsius by transforming the variable type from character to numeric and dividing each value by 10. Similarly, 'prcp' was converted from tenths of a millimeter to millimeters by dividing each value by 10.

```{r}
ny_noaa_tidy =
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    day = as.integer(day),
    year = as.factor(year),
    month = month.name[as.numeric(month)],
    month = factor(month, levels = month.name), 
    prcp = as.double(prcp / 10),
    tmax = as.integer(tmax),
    tmax = as.double(tmax / 10),
    tmin = as.integer(tmin),
    tmin = as.double(tmin / 10))
```

**For snowfall, what are the most commonly observed values? Why?**

```{r}
ny_noaa_tidy %>% 
  count(snow) %>% 
  arrange(desc(n))
```

The most commonly observed values for snowfall are 0 mm by far, followed by NA. This is because there are significantly more days in the year with no snow (snow = 0) than days with snow in New York. This dataset also contains extensive missing data, explaining why NA is the second most commonly observed value for snow.

### Part Two
The following code creates a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r fig.width = 10}
ny_noaa_tidy %>%
  filter(
    month %in% c("January","July")) %>% 
  group_by(id, month, year) %>% 
  summarize(
    tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = tmax_mean, group = id, color = id)) +
  geom_line(show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 60)) +
  facet_grid(.~month) +
  labs(
    title = "Mean max temperature over time",
    x = "Year",
    y = "Mean Max Temperature (C)")
```

**Interpretation**: As expected, values for mean max temperature are higher in July than in January for our data in New York. Values fluctuate over time for each month. In January, mean max temperature ranges from -10 to 10 degrees C, on average. In July, mean max temperature ranges from 20 to 35 degrees C, on average. There are a few outliers in each month, including in January 1982, 1993, 2000, and 2004 and in July 1984, 1988, 2004, and 2007.

### Part Three

The following code uses patchwork to create a two-panel plot showing (i) tmax vs tmin for the full dataset; and (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year. I will use `geom_hex` and `geom_density` for each plot, respectively.

```{r fig.height = 8}
tmax_tmin_p =
  ny_noaa_tidy %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  theme(legend.position = "right")

snowfall_p =
  ny_noaa_tidy %>%
  filter(
     snow > 0, snow < 100) %>% 
  group_by(id, month, year) %>% 
  ggplot(aes(x = snow, color = year)) +
  geom_density() +
  theme(legend.position = "right")

tmax_tmin_p / snowfall_p
```
